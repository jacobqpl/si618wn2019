{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 618: Data Manipulation and Analysis\n",
    "## 07 - Natural Language Processing\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a> This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline for today\n",
    "- announcements\n",
    "  - Teplovs Office Hours this week: 3:45-5:00pm\n",
    "  - no HTML,  no grade\n",
    "- regular expressions redux\n",
    "- ```spaCy```\n",
    "    - Cleaning the data\n",
    "    - Extracting linguistic features\n",
    "- ```Word2Vec```\n",
    "    - Vector representation of words\n",
    "    - Word similarities\n",
    "    - Vector algebra for semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Why learn NLP?\n",
    "- Natural language = human language\n",
    "- We use language to learn about the world\n",
    "- How machines understand human langauge?\n",
    "- How can we quantify the meaning of language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications?\n",
    "- Probabily any service that uses text as information\n",
    "- Search engine, SNS\n",
    "    - What's the document about?\n",
    "    - How do you determine the similarity?\n",
    "- Virtual assistants: Alexa, Google Assistant, Cortana, etc. \n",
    "    - Understand the semantic information from your speech from parsed text\n",
    "- Biology, genetics\n",
    "    - Genetic information / DNA sequence as text\n",
    "    - Draw networks of proteins/molecules from vast amount of scientific papers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we have included a [cheat sheet for regular expressions](python-regular-expressions-cheat-sheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are simply a way to find sequnces of characters within strings.  Let's use, as our text, \"Bereft\", a poem by Robert Frost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft = \"\"\"Where had I heard this wind before\n",
    "Change like this to a deeper roar?\n",
    "What would it take my standing there for,\n",
    "Holding open a restive door,\n",
    "Looking down hill to a frothy shore?\n",
    "Summer was past and the day was past.\n",
    "Sombre clouds in the west were massed.\n",
    "Out on the porch's sagging floor,\n",
    "Leaves got up in a coil and hissed,\n",
    "Blindly striking at my knee and missed.\n",
    "Something sinister in the tone\n",
    "Told me my secret my be known:\n",
    "Word I was in the house alone\n",
    "Somehow must have gotten abroad,\n",
    "Word I was in my life alone,\n",
    "Word I had no one left but God.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our lives simpler (we'll discuss why this is the case in class), we're going to strip the newlines from the passage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft = bereft.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to find all the occurrences of the word \"alone\".   We could use  plain old string functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft.find('alone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'alone' in bereft:\n",
    "    print('Yup, found it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft.count('alone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now try it yourself:  \n",
    "### <font color=\"magenta\">Q1: How many times does the word ```was``` appear in the poem?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good.  Now let's make things a bit more interesting.  How many words are there that contain the letters ```one``` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft.count('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('one',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we wanted to know the words that contained ```one``` instead of just the count?  Enter regular expressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[a-z]+one',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful online resources:\n",
    "\n",
    "* www.debuggex.com\n",
    "* www.regexr.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('[a-z]*one',bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search('[a-z]*one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Groups\n",
    "\n",
    "In the above example, we used ```match.group(0)``` to extract the entire match.\n",
    "\n",
    "Match groups also allow you to extract only certain parts of the match.  In the previous example, say we wanted to know which letters preceded the letters 'one'.  We could use match groups, specified by paretheses, to extract only certain parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search('([a-z]*)one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")\n",
    "    print(match.group(0))\n",
    "    print(match.group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we extract all the letters that precede *one*?  Use ```re.finditer()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.finditer('([a-z]*)one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(match.group(0),match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(',',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2: Experiment with various regular expressions such as \\W, \\w, \\s, \\S to see how the poem can be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about a few rounds of regex golf?\n",
    "\n",
    "### <font color=\"magenta\">Q3: See how well you can do at your tables: https://alf.nu/RegexGolf</a></font>\n",
    "Record your final score below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying regex to pandas DataFrames (from last class) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's load up some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/amazon_food_reviews.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a really small sample, just so we can experiment with the various "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample = reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some basic string functionality from Pandas that can be applied to any Series or Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.ProfileName.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.ProfileName.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.Summary.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ```columns``` attribute of a DataFrame is an Index object, which means that we can use str operators on the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the \"User Id\" column of the dataframe looks weird:  it has a space in the middle *and* at the end.  Columns that are named like that will invariable trip us up in downstream (i.e. later) analyses, so it's wise to correct them now.  Something like the following can help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.columns.str.strip().str.lower().str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can assign that back to the columns attribute to actually rename the columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.columns = reviews_sample.columns.str.strip().str.lower().str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Replacing Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to split strings into lists.  We might want to do that with the \"summary\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.productid.str.split('00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.productid.str.split('00').str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.productid.str.split('00').str[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace (regex time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.lower().str.replace('dog','health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.lower().str.replace('dog|taffy','health')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.extract(r'(Dog)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.extract(r'(Dog|Taffy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.extract(r'(Dog|[Tt]affy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a Series\n",
    "reviews_sample.summary.str.extract(r'(Dog|[Tt]affy)', expand = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.extractall(r'(Dog|[Tt]affy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.summary.str.extractall(r'(as)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Strings that Match or Contain a Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[Gg]ood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.text.str.contains(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.text.str.match(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'.*[Gg]ood.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample.text.str.match(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful resources:\n",
    "- Pandas text documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html\n",
    "- Regex Cheat Sheet: https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: How many rows from the Amazon Food Reviews data set contain HTML tags in the ```text``` column?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: Remove all HTML tags from the Amazon Food Reviews text column and save the results to a column called text_no_html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Install the spaCy and gensim libraries now.  Windows users will need to implement some work-arounds to get spaCy to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fast, and extensible NLP package for Python\n",
    "- <https://spacy.io/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to do this only once\n",
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading up the language model: English\n",
    "# Windows users will need to follow the instructions in Slack to modify the next line\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://en.wikipedia.org/wiki/Portal:History\n",
    "sentences = \"\"\"History (from Greek ἱστορία, historia, meaning \"inquiry, knowledge acquired by investigation\") is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.\n",
    "\n",
    "History can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing \"perspective\" on the problems of the present.\n",
    "\n",
    "Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the \"disinterested investigation\" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the \"father of history\", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd-century BC texts survived.\n",
    "\n",
    "Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section goal: calculate the frequency of each word\n",
    "- See which words are more frequent.\n",
    "- Generate more meaningful summary for the above paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowering the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_low = sentences.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Removing punctuation and special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude special characters one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.programiz.com/python-programming/examples/remove-punctuation\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''' # list of special characters you want to exclude\n",
    "sent_low_pnct = \"\"\n",
    "for char in sent_low:\n",
    "    if char not in punctuations:\n",
    "        sent_low_pnct = sent_low_pnct + char\n",
    "\n",
    "sent_low_pnct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, we can use regular expression to remove punctuation:\n",
    "- So we don't have to list up all possible special characters that we want to remove\n",
    "- https://docs.python.org/3.4/library/re.html\n",
    "- https://en.wikipedia.org/wiki/Regular_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent_low_pnct2 = re.sub(r'[^\\w\\s]', '', sent_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_low_pnct2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, special character ```\\n``` (linebreak) still exists in both cases. Let's remove these additionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.linesep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_low_pnct = sent_low_pnct.replace(os.linesep, \"\")\n",
    "sent_low_pnct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\rightarrow$ 3 possible ways to replace characters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q6. How would you remove numbers from the paragraph? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stop words usually refers to the most common words in a language\n",
    "    - No single universial stopwords\n",
    "    - Often stopwords are removed to improve the performance of NLP models\n",
    "    - https://en.wikipedia.org/wiki/Stop_words\n",
    "    - https://en.wikipedia.org/wiki/Most_common_words_in_English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the list of stop words from ```spaCy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: We are going to count the frequency of each word from the paragraph, to see which words can be used to represent the paragraph's content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we do not remove stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that our paragraph is stored as a single string object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_low_pnct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the paragraph into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sent_low_pnct.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words from the list\n",
    "- Words that can occur in any kind of paragraphs...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(45,10))\n",
    "sns.countplot(words, order=pd.Series(words).value_counts().index)\n",
    "# sns.countplot(words_nostop, order=[counted[0] for counted in Counter(words_nostop).most_common()])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click the plot to enlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we removed stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentence into words\n",
    "words_nostop = list()\n",
    "for word in words:\n",
    "    if word not in STOP_WORDS:\n",
    "        words_nostop.append(word)\n",
    "# words_nostop = [word for word in words if word not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More comprehensible, and unique list or words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words_nostop).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(45,10))\n",
    "sns.countplot(words_nostop, order=pd.Series(words_nostop).value_counts().index)\n",
    "# sns.countplot(words_nostop, order=[counted[0] for counted in Counter(words_nostop).most_common()])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click the plot to enlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q7. Based on the word frequency results, what was the paragraph about? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting linguistic features from spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "- Token: a semantic unit for analysis\n",
    "    - (Loosely) equal term for word\n",
    "        - ```sent_low_pnct.split()```\n",
    "    - Tricky cases\n",
    "        - aren't $\\rightarrow$ ![](https://nlp.stanford.edu/IR-book/html/htmledition/img88.png) ![](https://nlp.stanford.edu/IR-book/html/htmledition/img89.png) ? ![](https://nlp.stanford.edu/IR-book/html/htmledition/img86.png) ?\n",
    "        - O'Neil $\\rightarrow$ ![](https://nlp.stanford.edu/IR-book/html/htmledition/img83.png) ? ![](https://nlp.stanford.edu/IR-book/html/htmledition/img84.png) ![](https://nlp.stanford.edu/IR-book/html/htmledition/img81.png) ?\n",
    "        - https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "- In ```spaCy```:\n",
    "    - Many token types, like word, puntuation symbol, whitespace, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dissect the sentence!\n",
    "\n",
    "- initiating the ```spaCy``` object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples partially taken from https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our sentence: \"Hello World!\"\n",
    "    - Pass the sentence string to the ```spaCy``` object ```nlp```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sentence is considered as a short document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(doc), doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As importing the sentence string above, ```spaCy``` splited the sentence into tokens (tokenization!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,token in enumerate(doc):\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With index information (location from the sentence) of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10| 11|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| H | e | l | l | o | _ | W | o | r | l | d | ! |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(doc):\n",
    "    print(i, token.text, token.idx) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And many more!\n",
    "    - https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"What did you do during the study break   ?\")\n",
    "\n",
    "print(\"text \\t idx \\t lemma \\t lower \\t is_punct \\t is_space \\t shape \\t POS\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.lower_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the document with multiple sentences, we would need to separate between each sentences.\n",
    "- In ```spaCy```, the job is more convinient (and would cause less mistatkes) than using regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q8. How would you separate sentences? What's your intuition? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our multiple sentence document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_multsent = \"These are apples. Those are oranges from N.Y.C. and...? How about pineapples? Not carrots!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in regular expression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", doc_multsent) # how would I remember this pattern without Google/StackOverflow?\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(i, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in ```spaCy```!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same document, but initiate as the spaCy object...\n",
    "doc = nlp(doc_multsent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentences are stored as a generator object\n",
    "    - Instead of storing sentences as a list, each sentence is stored as a item in the generator object \n",
    "    - Iteratable (i.e., can be used in a for loop)\n",
    "    - More efficient memory use\n",
    "    - https://wiki.python.org/moin/Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prining sentences with the index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(i, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q9. Separate sentences in the following paragraph, and print sentences with the index number. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://en.wikipedia.org/wiki/Portal:History\n",
    "sentences = \"\"\"History (from Greek ἱστορία, historia, meaning \"inquiry, knowledge acquired by investigation\") is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.\n",
    "\n",
    "History can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing \"perspective\" on the problems of the present.\n",
    "\n",
    "Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the \"disinterested investigation\" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the \"father of history\", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd-century BC texts survived.\n",
    "\n",
    "Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to find words with particular part-of-speech!\n",
    "- Different part-of-speech words carry different information\n",
    "    - e.g., noun (subject), verb (action term), adjective (quality of the object) \n",
    "- https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yelp review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.yelp.com/biz/ajishin-novi?hrid=juA4Zn2TX7845vNFn4syBQ&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)\n",
    "doc = nlp(\"\"\"One of the best Japanese restaurants in Novi. Simple food, great taste, amazingly price. I visit this place a least twice month.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q10a. What can you infer from this review? </font>\n",
    "- What type of the restaurant?\n",
    "- Location?\n",
    "- What did the reviewer liked about?\n",
    "- How often did the person visit the place?\n",
    "- Any other information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiple sentences exist in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(i, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: which words are adjective (ADJ)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(\"__sentence__:\", i)\n",
    "    print(\"_token_ \\t _POS_\")\n",
    "    for token in sent:\n",
    "        print(token.text, \"\\t\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramatical dependency\n",
    "- Words are gramatically related in a sentence.\n",
    "- Conveys much semantic information about the sentential context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And dependency relationships also can be extracted as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('\"' + token.text + '\", ', token.pos_, list(token.ancestors), (token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```spaCy``` follows the ```ClearNLP``` annotations for dependency parsing\n",
    "- https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q10b: (Now answer the same question based on the POS/dependency parsing results) What can you infer from this review? </font>\n",
    "- What type of the restaurant?\n",
    "- Location?\n",
    "- What did the reviewer liked about?\n",
    "- How often did the person visit the place?\n",
    "- Any other information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we seen how we can extract some interesting syntactic characteristics from text from using ```spacy```\n",
    "- It extracted the characteristics, but did not indicate what it means\n",
    "- Can machines understand semantic relationship between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distributional semantics\n",
    "    - Representing semantic information of words in a geometric semantic space\n",
    "        - Different relationship between words: explained by geometric relationship between words \n",
    "        - e.g., Related words are located closer to each other; \n",
    "    - And it's often called as *word embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "- Developed by [Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- Represent the meaning of the words as a vector\n",
    "    - Vector: numeric array\n",
    "    - Output of a neural network model that predicts the next word\n",
    "- Surprisingly, many different semantic informations can be represented from word vectors of ```Word2Vec```\n",
    "- (More explanation in here: https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/softmax-nplm.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with some example: words in a semantic space\n",
    "$\\rightarrow$ https://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q11. Record any interesting findings from TensorFlow Projector page</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK. Let's try some more details in our local machines!\n",
    "- Download the [pretrained model](https://drive.google.com/open?id=10GXpuviDJVa-k8ZmiYX3BVABNDRaA6tg) and place it in the same folder as this notebook\n",
    "- We are using [gensim](https://radimrehurek.com/gensim/) package this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -y gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/eyaler/word2vec-slim\n",
    "w2v_mod = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: What's similarity between *school* and *student*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the word vector for *school* looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_mod['school'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and the word vector for *student* looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod['student']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the similarity between two word vectors is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod.similarity('school', 'student')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for measuring similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_76-jun-10-08-36.jpg\" width=\"400\"></td>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_77-jun-10-08-36.jpg\" width=\"400\"></td>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_77-jun-10-08-37.jpg\" width=\"400\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euclidean distance\n",
    "    - The most common use of distance\n",
    "    - $ \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (images from https://nickgrattan.wordpress.com/2014/06/10/euclidean-manhattan-and-cosine-distance-measures-in-c/)\n",
    "np.sqrt(np.power((12-5), 2) + np.power((14-11), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manhattan distance\n",
    "    - Distance = the sum of differences in the grid\n",
    "    - $|x_1 - x_2| + |y_1 - y_2|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(12-5) + np.abs(14-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity \n",
    "    - Often used to measure similarity between vectors\n",
    "    - $cos(\\theta) = \\frac{\\sum_{i=1}^{n} A_i B_i }{\\sqrt{\\sum_{i=1}^{n} A_i^2 } \\sqrt{\\sum_{i=1}^{n} B_i^2 }}$ \n",
    "    - https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([12, 14])\n",
    "b = np.array([5, 11])\n",
    "a.dot(b) / (np.sqrt(np.sum(np.power(a, 2))) * np.sqrt(np.sum(np.power(b, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (image from http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine simiarity can go from -1 to 1\n",
    "- But usually, we deal with 0 to 1 scores for comparing words in ```Word2Vec```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q12a. What's the cosine similarity between *school* and *tiger*? </font>\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q12b. Try some other words. Any other interesting findings? </font>\n",
    "- Give 3 more examples.\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy from word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we approximate the relationship between words by doing - and + operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $woman - man + king \\approx ?$\n",
    "- How this works?\n",
    "    - $woman:man \\approx x:king $\n",
    "    - $\\rightarrow woman - man \\approx x - king $\n",
    "    - $\\rightarrow woman - man + king \\approx x$\n",
    "    - List top-10 words ($x$) that can solve the equation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Spain - Germany + Berlin \\approx ?$\n",
    "    - $\\rightarrow Spain - Germany \\approx x -  Berlin $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod.most_similar(positive=['Spain', 'Berlin'], negative=['Germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q13. Any other interesting examples? </font>\n",
    "- Give 3 more examples.\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing interpretable semantic scales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we saw that word vectors effectively carries (although not perfect) the semantic information.\n",
    "- Can we design something more interpretable results from using the semantic space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's re-try with real datapoints in [here](https://projector.tensorflow.org): *politics* words in a *bad-good* PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "def cosine_similarity(x, y):\n",
    "    return(1 - spatial.distance.cosine(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we regenerate this results with our embedding model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot words in the 2D space\n",
    "- Using Bad & Good axes\n",
    "- Calculate cosine similarity between an evaluating word (violence, discussion, and issues) with each scale's end (bad, and good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim_2d = pd.DataFrame([[cosine_similarity(w2v_mod['violence'], w2v_mod['good']), cosine_similarity(w2v_mod['violence'], w2v_mod['bad'])],\n",
    "                                 [cosine_similarity(w2v_mod['discussion'], w2v_mod['good']), cosine_similarity(w2v_mod['discussion'], w2v_mod['bad'])],\n",
    "                                 [cosine_similarity(w2v_mod['issues'], w2v_mod['good']), cosine_similarity(w2v_mod['issues'], w2v_mod['bad'])]],\n",
    "                                index=['violence', 'discussion', 'issues'], columns=['good', 'bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='good', y='bad', data=pol_words_sim_2d, hue=pol_words_sim_2d.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- violence: less good, more bad\n",
    "- discussion: less bad, more good\n",
    "- issues: both bad and good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do this in an 1D scale?\n",
    "(bad) --------------------?---- (good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, let's create the vector for *bad-good* scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_bad_good = w2v_mod['good'] - w2v_mod['bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the cosine similarity score of the word *violence* in the *bad-good* scale \n",
    "    - $sim(V(violence), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(w2v_mod['violence'], scale_bad_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $sim(V(discussion), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(w2v_mod['discussion'], w2v_mod['good'] - w2v_mod['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $sim(V(issues), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(w2v_mod['issues'], w2v_mod['good'] - w2v_mod['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, as displayed in Embedding Projector, words *violence*, *discussion*, and *issues* are located in the *bad-good* scale as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim = pd.DataFrame([cosine_similarity(w2v_mod['violence'], w2v_mod['good'] - w2v_mod['bad']),\n",
    "                              cosine_similarity(w2v_mod['discussion'], w2v_mod['good'] - w2v_mod['bad']),\n",
    "                              cosine_similarity(w2v_mod['issues'], w2v_mod['good'] - w2v_mod['bad'])],\n",
    "                             index=['violence', 'discussion', 'issues'], columns=['cos_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=pol_words_sim.index, y=pol_words_sim.cos_sim)\n",
    "ax.set(ylabel=\"bad_good scale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Violence* is more close to the *bad* end of the scale, while *discussion* is located towards the *good* end of the scale. *Issues* is located between those two words in the *bad-good* scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q14.  Select different scale and a set of words. How words are represented in the your suggested semantic scale? </font>\n",
    "- Why did you selected the particular scale and words? what's your interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- more to read about this method:     http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
